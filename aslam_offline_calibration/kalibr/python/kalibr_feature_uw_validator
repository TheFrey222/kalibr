#!/usr/bin/env python
import rospy
from sensor_msgs.msg import Image, CompressedImage
from cv_bridge import CvBridge, CvBridgeError
import message_filters

import cv2
import sm
import aslam_cv as acv
import aslam_cameras_april as acv_april
import kalibr_common as kc

import time
import numpy as np
import argparse
import igraph
from math import dist, isnan
from scipy.spatial.transform import Rotation as R

import superpoint as sp


# make numpy print prettier
np.set_printoptions(suppress=True)

def get_disparity_vis(src: np.ndarray, scale: float = 1.0) -> np.ndarray:
    '''Replicated OpenCV C++ function

    Found here: https://github.com/opencv/opencv_contrib/blob/b91a781cbc1285d441aa682926d93d8c23678b0b/modules/ximgproc/src/disparity_filters.cpp#L559
    
    Arguments:
        src (np.ndarray): input numpy array
        scale (float): scale factor

    Returns:
        dst (np.ndarray): scaled input array
    '''
    dst = (src * scale/16.0).astype(np.uint8)
    return dst

class CalibrationTargetDetector(object):
    def __init__(self, camera, targetConfig):

        targetParams = targetConfig.getTargetParams()
        targetType = targetConfig.getTargetType()
        
        #set up target
        if( targetType == 'checkerboard' ):
            grid = acv.GridCalibrationTargetCheckerboard(targetParams['targetRows'], 
                                                            targetParams['targetCols'], 
                                                            targetParams['rowSpacingMeters'], 
                                                            targetParams['colSpacingMeters'])
        
        elif( targetType == 'circlegrid' ):
            options = acv.CirclegridOptions(); 
            options.useAsymmetricCirclegrid = targetParams['asymmetricGrid']
            
            grid = acv.GridCalibrationTargetCirclegrid(targetParams['targetRows'],
                                                          targetParams['targetCols'], 
                                                          targetParams['spacingMeters'], 
                                                          options)
        
        elif( targetType == 'aprilgrid' ):
            grid = acv_april.GridCalibrationTargetAprilgrid(targetParams['tagRows'], 
                                                            targetParams['tagCols'], 
                                                            targetParams['tagSize'], 
                                                            targetParams['tagSpacing'])
        else:
            raise RuntimeError( "Unknown calibration target." )
        
        #setup detector
        options = acv.GridDetectorOptions() 
        options.filterCornerOutliers = True
        self.detector = acv.GridDetector(camera.geometry, grid, options)
    
class CameraChainValidator(object):
    def __init__(self, chainConfig, targetParams, feature):

        self.cammatrix = np.zeros((3,3))
        self.feature = feature
        print(f"Selected feature detector: {feature}")
        self.fe = sp.superpoint_frontend.SuperPointFrontend(
            weights_path="/home/lian-li/catkin_ws/kalibr_ws/src/kalibr/aslam_offline_calibration/kalibr/python/superpoint/superpoint_v1.pth",
            nms_dist=4,
            conf_thresh=0.015,
            nn_thresh=0.7,
            cuda=True)
        self.matches_total = 0
        self.outliers = 0
        self.images_total = 1
        
        self.chainConfig = chainConfig
        self.numCameras = chainConfig.numCameras()
        self.bridge = CvBridge()

        self.Pna = np.zeros((3,4))
        self.Pnb = np.zeros((3,4))
        
        #initialize the cameras in the chain
        self.G = igraph.Graph(self.numCameras)
        self.monovalidators = []
        for cidx in range(0, self.numCameras):
            camConfig = chainConfig.getCameraParameters(cidx)
            
            #create a mono instance for each cam (detection and mono view)
            monovalidator = MonoCameraValidator(camConfig, targetParams)
            cv2.namedWindow(monovalidator.windowName, cv2.WINDOW_NORMAL)
            cv2.resizeWindow(monovalidator.windowName, (640, 480))
            self.monovalidators.append(monovalidator)
            
            #add edges to overlap graph
            overlaps=chainConfig.getCamOverlaps(cidx)
            for overlap in overlaps:
                #add edge if it isn't existing yet
                try:
                    edge_idx = self.G.get_eid(cidx, overlap)
                except:
                    self.G.add_edges([(cidx, overlap)])

        #prepare the rectification maps
        for edge in self.G.es:
            cidx_src = edge.source
            cidx_dest = edge.target
            edge["rect_map"] = dict()
            edge["R"] = dict()
            edge["A"] = dict()
            edge["rect_map"][cidx_src], \
            edge["rect_map"][cidx_dest], \
            edge["R"][cidx_src], \
            edge["R"][cidx_dest], \
            edge["A"][cidx_src], \
            edge["A"][cidx_dest] = self.prepareStereoRectificationMaps(cidx_src, cidx_dest)

        # generate the windows
        for edge in self.G.es:
            cidx_src = edge.source
            cidx_dest = edge.target
            windowName = "Rectified view (cam{0} and cam{1})".format(cidx_src, cidx_dest)
            cv2.namedWindow(windowName, cv2.WINDOW_NORMAL)
            cv2.resizeWindow(windowName, (2*640, 480))
                
        #register the callback for the synchronized images
        sync_sub = message_filters.ApproximateTimeSynchronizer([val.image_sub for val in self.monovalidators], 10, 0.02)
        sync_sub.registerCallback(self.synchronizedCallback)

        #initialize message throttler
        self.timeLast = 0

    def synchronizedCallback(self, *cam_msgs):
        #throttle image processing
        rate = 2 #Hz
        timeNow = time.time()
        if (timeNow-self.timeLast < 1.0/rate) and self.timeLast!=0:
            return
        self.timeLast = timeNow
         
        #process the images of all cameras
        self.observations=[]
        for cam_nr, msg in enumerate(cam_msgs):
          
            #convert image to gray single channel numpy array
            try:
                if msg._type == 'sensor_msgs/CompressedImage':
                    np_image = np.array(self.bridge.compressed_imgmsg_to_cv2(msg))
                    if len(np_image.shape) > 2 and np_image.shape[2] == 3:
                        np_image = cv2.cvtColor(np_image, cv2.COLOR_BGR2GRAY)
                elif msg.encoding == "rgb8" or msg.encoding == "bgr8":
                    np_image = np.array(self.bridge.imgmsg_to_cv2(msg, "mono8"))
                else:
                    np_image = np.array(self.bridge.imgmsg_to_cv2(msg))
            except CvBridgeError as e:
                print("Could not convert ros message to opencv image: ", e)
                continue
            
            #get the corresponding monovalidator instance
            validator = self.monovalidators[cam_nr]
            
            #detect targets for all cams
            timestamp = acv.Time(msg.header.stamp.secs, msg.header.stamp.nsecs)
            success, observation = validator.target.detector.findTarget(timestamp, np_image)
            observation.clearImage()
            validator.obs = observation
            
            #undistort the image
            if type(validator.camera.geometry) == acv.DistortedOmniCameraGeometry:
                validator.undist_image = validator.undistorter.undistortImageToPinhole(np_image)
            else:
                validator.undist_image = validator.undistorter.undistortImage(np_image)
                            
            #generate a mono view for each cam
            #validator.generateMonoview(np_image, observation, success)            
            
        #generate all rectification views
        for edge in self.G.es:
            cidx_src = edge.source
            cidx_dest = edge.target
            self.generatePairView(cidx_src, cidx_dest)
                
        cv2.waitKey(1)
    
    #returns transformation T_to_from
    def getTransformationCamFromTo(self, cidx_from, cidx_to):
        #build pose chain (target->cam0->baselines->camN)
        lowid = min((cidx_from, cidx_to))
        highid = max((cidx_from, cidx_to))
        
        T_high_low = sm.Transformation()
        for cidx in range(lowid, highid):
            baseline_HL = self.chainConfig.getExtrinsicsLastCamToHere(cidx+1)
            T_high_low = baseline_HL * T_high_low
        
        if cidx_from<cidx_to:
            T_BA = T_high_low
        else:
            T_BA = T_high_low.inverse()
        
        return T_BA

    def rectifyImages(self, imageA, imageB, mapA, mapB):
        #rectify images
        rect_image_A = cv2.remap(imageA,
                                 mapA[0],
                                 mapA[1],
                                 cv2.INTER_LINEAR)

        rect_image_B = cv2.remap(imageB,
                                 mapB[0],
                                 mapB[1],
                                 cv2.INTER_LINEAR)
               
        return rect_image_A, rect_image_B

    def generatePairView(self, camAnr, camBnr):

        #get the mono validators for each cam
        camA = self.monovalidators[camAnr]
        camB = self.monovalidators[camBnr]
        
        #get baseline between camA & camB
        T_BA = self.getTransformationCamFromTo(camAnr, camBnr)
        
        #extract the common corners for the camera in pair
        keypoints_A = camA.obs.getCornersImageFrame()
        keypoints_A_id = camA.obs.getCornersIdx()
        keypoints_B = camB.obs.getCornersImageFrame()
        keypoints_B_id = camB.obs.getCornersIdx()
        targetPoints = camA.obs.getCornersTargetFrame()
        
        #get the common corners
        common_ids = set(keypoints_A_id) & set(keypoints_B_id)
        
        #project points from camera A to cam B to compare with detection
        #and vice versa
        if len(common_ids) > 0:
            #add to new list
            commonKeypoints_A=[]; commonKeypoints_B=[]; reprojs_A=[]; reprojs_B=[]
            commonTargetPoints=[]; reproj_errs=np.array([0,0])
            
            for id in common_ids:
                commonKeypoints_A.append( keypoints_A[np.where(keypoints_A_id==id)] )
                commonKeypoints_B.append( keypoints_B[np.where(keypoints_B_id==id)] )
                commonTargetPoints.append( targetPoints[np.where(keypoints_A_id==id)] )
            

            for keypoint_A, keypoint_B, targetPoint in zip(commonKeypoints_A, commonKeypoints_B, commonTargetPoints):
                #reproject
                T_AW = camA.obs.T_t_c().inverse()
                T_BW_base = T_BA*T_AW
                reprojB = np.matrix(T_BW_base.C()) * np.matrix(targetPoint).reshape(3,1) + np.matrix(T_BW_base.t()).reshape(3,1)
                reprojErr_B = camB.camera.geometry.euclideanToKeypoint(reprojB) - keypoint_B
                
                T_BW = camB.obs.T_t_c().inverse()
                T_AW_base = T_BA.inverse()*T_BW
                reprojA = np.matrix(T_AW_base.C()) * np.matrix(targetPoint).reshape(3,1) + np.matrix(T_AW_base.t()).reshape(3,1)
                reprojErr_A = camA.camera.geometry.euclideanToKeypoint(reprojA) - keypoint_A
                
                #reprojection errors in original camera geomtery
                reproj_errs = np.hstack((reproj_errs, reprojErr_A.flatten()))
                reproj_errs = np.hstack((reproj_errs, reprojErr_B.flatten()))     
                
            reproj_errs = reproj_errs[2:]
            reproj_errs = reproj_errs.reshape(2, int(reproj_errs.shape[0]/2.0))
                    
        #rectify the undistorted images
        edge_idx = self.G.get_eid(camAnr, camBnr)
        edge = self.G.es[edge_idx]
        A_rect = edge["A"][camAnr] #some for cam B
        
        rect_image_A, rect_image_B = self.rectifyImages(camA.undist_image, 
                                          camB.undist_image,
                                          edge["rect_map"][camAnr],
                                          edge["rect_map"][camBnr])

        if self.feature == "save":
            cv2.imwrite(f'/mnt/c/Users/Andre/Documents/MATLAB/ssd-disparity-map/input/left/{self.images_total}.png', rect_image_A)
            cv2.imwrite(f'/mnt/c/Users/Andre/Documents/MATLAB/ssd-disparity-map/input/right/{self.images_total}.png', rect_image_B)
            self.images_total += 1
            return

        good = []
        good_ = []
        kp1 = []
        kp2 = []
        kp1_ = []
        kp2_ = []

        if self.feature == "superpoint":
            rect_image_A_ = rect_image_A.astype('float32') / 255.
            rect_image_B_ = rect_image_B.astype('float32') / 255.
            kp1, desc1, heatmap1 = self.fe.run(rect_image_A_)
            kp2, desc2, heatmap2 = self.fe.run(rect_image_B_)

            assert desc1.shape[0] == desc2.shape[0]
            if desc1.shape[1] == 0 or desc2.shape[1] == 0:
                return
            if self.fe.nn_thresh < 0.0:
                raise ValueError('\'nn_thresh\' should be non-negative')

            # Compute L2 distance. Easy since vectors are unit normalized.
            dmat = np.dot(desc1.T, desc2)
            dmat = np.sqrt(2-2*np.clip(dmat, -1, 1))

            # Get NN indices and scores.
            idx = np.argmin(dmat, axis=1)
            scores = dmat[np.arange(dmat.shape[0]), idx]

            # Threshold the NN matches.
            keep = scores < self.fe.nn_thresh

            # Check if nearest neighbor goes both directions and keep those.
            idx2 = np.argmin(dmat, axis=0)
            keep_bi = np.arange(len(idx)) == idx2[idx]
            keep = np.logical_and(keep, keep_bi)
            idx = idx[keep]
            scores = scores[keep]

            # Get the surviving point indices.
            m_idx1 = np.arange(desc1.shape[1])[keep]
            m_idx2 = idx

            # Populate the final 3xN match data structure.
            for i in range(len(m_idx1)):
                good.append([cv2.DMatch(m_idx1[i], m_idx2[i], scores[i])])
                kp1_.append(kp1[m_idx1[i]].pt)
                kp2_.append(kp2[m_idx2[i]].pt)

        elif self.feature == "dense":
            left_matcher = 	cv2.StereoBM.create()
            wls_filter = cv2.ximgproc.createDisparityWLSFilter(left_matcher)
            right_matcher = cv2.ximgproc.createRightMatcher(left_matcher)
            left_disp = left_matcher.compute(rect_image_A, rect_image_B)
            right_disp = right_matcher.compute(rect_image_B, rect_image_A)

            disp_map_vis = get_disparity_vis(left_disp)
            windowName = "Disparity map"
            cv2.imshow(windowName, disp_map_vis)

            left_disp_fil = left_disp
            wls_filter.filter(left_disp, camA.undist_image, left_disp_fil, right_disp)

            disp_map_vis = get_disparity_vis(left_disp_fil)
            windowName = "Disparity map filtered"
            cv2.imshow(windowName, disp_map_vis)

            return

        else:
            # Initiate detector
            if self.feature == "akaze":
                orb = cv2.AKAZE_create()
            elif self.feature == "kaze":
                orb = cv2.KAZE_create()
            elif self.feature == "brisk":
                orb = cv2.BRISK_create()
            elif self.feature == "sift":
                orb = cv2.SIFT_create()
            elif self.feature == "surf":
                orb = cv2.xfeatures2d.SURF_create(300, nOctaveLayers=2)
            elif self.feature == "orb":
                orb = cv2.ORB_create()
            else:
                print("Wrong feature input...")
                return
        
            # find the keypoints and descriptors with ORB
            kp1, des1 = orb.detectAndCompute(rect_image_A,None)
            kp2, des2 = orb.detectAndCompute(rect_image_B,None)

            match_len = 0

            if len(kp1) < 6 or len(kp2) < 6:
                print("Not enough keypoints...")
                m_per_img = int(self.matches_total/self.images_total)
                o_per_img = int(self.outliers/self.images_total)
                self.images_total += 1
                np_image_rect_gray = np.hstack( (rect_image_A, rect_image_B) )
                np_image_rect = cv2.cvtColor(np_image_rect_gray,cv2.COLOR_GRAY2RGB)
                cv2.putText(np_image_rect, "Not enough keypoints...", (int(np_image_rect.shape[0]/2.0),int(np_image_rect.shape[1]/5.0)), cv2.FONT_HERSHEY_SIMPLEX, fontScale=1.0, color=(0, 0, 255), thickness=2)
                self.showWindow(np_image_rect, o_per_img, m_per_img, camAnr, camBnr)
                return
            
            # create BFMatcher object
            bf = cv2.BFMatcher()

            # Match descriptors.
            matches = bf.knnMatch(des1,des2,k=2)

            # Apply ratio test
            for m,n in matches:
                if m.distance < 0.8*n.distance:
                    good.append([m])
                    kp1_.append(kp1[m.queryIdx].pt)
                    kp2_.append(kp2[m.trainIdx].pt)

        if len(good) < 4:
            print("Not enough matches...")
            good_ = good
            m_per_img = int(self.matches_total/self.images_total)
            o_per_img = int(self.outliers/self.images_total)
            self.images_total += 1
            np_image_rect = cv2.drawMatchesKnn(rect_image_A,kp1,rect_image_B,kp2,good_[:50],None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)
            cv2.putText(np_image_rect, "Not enough matches...", (int(np_image_rect.shape[0]/2.0),int(np_image_rect.shape[1]/5.0)), cv2.FONT_HERSHEY_SIMPLEX, fontScale=1.0, color=(0, 0, 255), thickness=2)
            self.showWindow(np_image_rect, o_per_img, m_per_img, camAnr, camBnr)
            return

        kp1__ = np.array(kp1_)
        kp2__ = np.array(kp2_)

        # Apply homography test
        reprojectionError = 1 # default value, you can change it to some lower to get more reliable estimation.
        H, mask = cv2.findHomography(kp1__, kp2__, cv2.FM_RANSAC, reprojectionError)

        if (H is None):
            print("Homography not found...")
            good_ = good
            m_per_img = int(self.matches_total/self.images_total)
            o_per_img = int(self.outliers/self.images_total)
            self.images_total += 1
            np_image_rect = cv2.drawMatchesKnn(rect_image_A,kp1,rect_image_B,kp2,good_[:50],None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)
            cv2.putText(np_image_rect, "Homography not found...", (int(np_image_rect.shape[0]/2.0),int(np_image_rect.shape[1]/5.0)), cv2.FONT_HERSHEY_SIMPLEX, fontScale=1.0, color=(0, 0, 255), thickness=2)
            self.showWindow(np_image_rect, o_per_img, m_per_img, camAnr, camBnr)
            return

        reprojectionError_y = 5
        outliers_len = 0

        for g in good:
            p1 = kp1[g[0].queryIdx].pt
            p2 = kp2[g[0].trainIdx].pt
            p1_repr = cv2.perspectiveTransform(np.array([[[p1[0],p1[1]]]]), H)
            p1_p1_repr = int(dist(p2, p1_repr[0][0]))
            p1_p2_y = int(abs(p1[1] - p2[1]))

            if p1_p1_repr > reprojectionError:
                outliers_len += 1
                print(f"Removed point with reprojection error of {p1_p1_repr} pixels")
            elif p1_p2_y > reprojectionError_y:
                outliers_len += 1
                print(f"Removed point with height difference of {p1_p2_y} pixels")
            else:
                good_.append([g[0]])

        if len(good_) < 4:
            print("Not enough matches...")
            m_per_img = int(self.matches_total/self.images_total)
            o_per_img = int(self.outliers/self.images_total)
            self.images_total += 1
            np_image_rect = cv2.drawMatchesKnn(rect_image_A,kp1,rect_image_B,kp2,good_[:50],None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)
            cv2.putText(np_image_rect, "Not enough matches...", (int(np_image_rect.shape[0]/2.0),int(np_image_rect.shape[1]/5.0)), cv2.FONT_HERSHEY_SIMPLEX, fontScale=1.0, color=(0, 0, 255), thickness=2)
            self.showWindow(np_image_rect, o_per_img, m_per_img, camAnr, camBnr)
            return

        match_len = len(good_)
        self.matches_total += match_len
        self.outliers += outliers_len
        m_per_img = int(self.matches_total/self.images_total)
        o_per_img = int(self.outliers/self.images_total)
        self.images_total += 1
        np_image_rect = cv2.drawMatchesKnn(rect_image_A,kp1,rect_image_B,kp2,good_[:50],None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)
        self.showWindow(np_image_rect, o_per_img, m_per_img, camAnr, camBnr)

        print("\nNumber of matches:  {}".format(match_len))
        print("Number of outliers: {} \n".format(outliers_len))

    def showWindow(self, np_image_rect, o_per_img, m_per_img, camAnr, camBnr):
        cv2.putText(np_image_rect, f"Outliers/images: {o_per_img}", (20,20), cv2.FONT_HERSHEY_SIMPLEX, fontScale=.7, color=(0, 0, 255), thickness=2)
        cv2.putText(np_image_rect, f"Matches/images: {m_per_img}", (20,50), cv2.FONT_HERSHEY_SIMPLEX, fontScale=.7, color=(0, 0, 255), thickness=2)
        cv2.putText(np_image_rect, f"Total matches: {self.matches_total}", (20,80), cv2.FONT_HERSHEY_SIMPLEX, fontScale=.7, color=(0, 0, 255), thickness=2)
        cv2.putText(np_image_rect, self.feature, (20, int(np_image_rect.shape[0] - 20)), cv2.FONT_HERSHEY_SIMPLEX, fontScale=.7, color=(0, 0, 255), thickness=2)
        windowName = "Rectified view (cam{0} and cam{1})".format(camAnr, camBnr)
        cv2.imshow(windowName, np_image_rect)
    
    def prepareStereoRectificationMaps(self, camAnr, camBnr):        
        #get the camera parameters for the undistorted cameras
        camIdealA = self.monovalidators[camAnr].undist_camera.projection().getParameters().flatten()
        camIdealB = self.monovalidators[camBnr].undist_camera.projection().getParameters().flatten()
        camIdealA = np.array([[camIdealA[0],0,camIdealA[2]], [0,camIdealA[1],camIdealA[3]], [0,0,1]])
        camIdealB = np.array([[camIdealB[0],0,camIdealB[2]], [0,camIdealB[1],camIdealB[3]], [0,0,1]])
        imageSize = (self.monovalidators[camAnr].undist_camera.projection().ru(), self.monovalidators[camAnr].undist_camera.projection().rv())
        
        #get the baseline between the cams       
        baseline_BA = self.getTransformationCamFromTo(camAnr, camBnr)
        
        ##
        #A.Fusiello, E. Trucco, A. Verri: A compact algorithm for recification of stereo pairs, 1999
        ##
        Poa = np.matrix(camIdealA) * np.hstack( (np.matrix(np.eye(3)), np.matrix(np.zeros((3,1)))) ) #use camA coords as world frame...
        Pob = np.matrix(camIdealB) * np.hstack( (np.matrix(baseline_BA.C()), np.matrix(baseline_BA.t()).T) )
        
        #optical centers (in camA's coord sys)
        c1 = -np.linalg.inv(Poa[:,0:3]) * Poa[:,3]
        c2 = -np.linalg.inv(Pob[:,0:3]) * Pob[:,3]
        
        #get "mean" rotation between cams        
        old_z_mean = (baseline_BA.C()[2,:].flatten()+sm.Transformation().T()[2,0:3])/2.0
        v1 = c1-c2 #newx-axis = direction of baseline        
        v2 = np.cross(np.matrix(old_z_mean).flatten(), v1.flatten()).T #new y axis orthogonal to new x and mean old z
        v3 = np.cross(v1.flatten(), v2.flatten()).T #orthogonal to baseline and new y
        
        #normalize
        v1 = v1/np.linalg.norm(v1)
        v2 = v2/np.linalg.norm(v2)
        v3 = v3/np.linalg.norm(v3)
        
        #create rotation matrix
        R = np.hstack((np.hstack((v1,v2)),v3)).T
        
        #new intrinsic parameters
        A = (camIdealA + camIdealB)/2.0

        self.cammatrix = A
        
        #new projection matrices
        self.Pna = A * np.hstack((R, -R*c1))
        self.Pnb = A * np.hstack((R, -R*c2))
        
        #rectyfing transforms
        Ta = self.Pna[0:3,0:3] * np.linalg.inv(Poa[0:3,0:3])
        Tb = self.Pnb[0:3,0:3] * np.linalg.inv(Pob[0:3,0:3])
        
        Ra = R #camA=world, then to rectified coords
        Rb = R * baseline_BA.inverse().C() #to world then to rectified coords
        
        
        #create the rectification maps
        rect_map_x_a, rect_map_y_a = cv2.initUndistortRectifyMap(camIdealA, 
                                                                 np.zeros((4,1)), 
                                                                 Ra, 
                                                                 A, 
                                                                 imageSize, 
                                                                 cv2.CV_16SC2)
        
        rect_map_x_b, rect_map_y_b = cv2.initUndistortRectifyMap(camIdealB, 
                                                                 np.zeros((4,1)), 
                                                                 Rb, 
                                                                 A, 
                                                                 imageSize,
                                                                 cv2.CV_16SC2)
        
        return (rect_map_x_a, rect_map_y_a), (rect_map_x_b, rect_map_y_b), Ra, Rb, A, A
    
        
class MonoCameraValidator(object):
    def __init__(self, camConfig, targetParams):
        
        self.total_mean_reprojection_error_x = 0
        self.total_std_reprojection_error_x = 0
        self.total_mean_reprojection_error_y = 0
        self.total_std_reprojection_error_y = 0
        self.total_images = 0

        print("initializing camera geometry")
        self.camera = kc.ConfigReader.AslamCamera.fromParameters(camConfig)
        self.target = CalibrationTargetDetector(self.camera, targetConfig)

        #print details
        print("Camera {0}:".format(camConfig.getRosTopic()))
        camConfig.printDetails()
        
        self.topic = camConfig.getRosTopic()
        self.windowName = "Camera: {0}".format(self.topic)
    
        #register the cam topic to the message synchronizer
        if "compressed" in self.topic:
            self.image_sub = message_filters.Subscriber(self.topic, CompressedImage)
        else:
            self.image_sub = message_filters.Subscriber(self.topic, Image)
        
        #create image undistorter
        alpha = 1.0
        scale = 1.0
        self.undistorter = self.camera.undistorterType(self.camera.geometry, cv2.INTER_LINEAR, alpha, scale)
        
        if type(self.camera.geometry) == acv.DistortedOmniCameraGeometry:
            #convert omni image to pinhole image aswell
            self.undist_camera = self.undistorter.getIdealPinholeGeometry()
        else:
            self.undist_camera = self.undistorter.getIdealGeometry()
            
        #storage for reproj errors
        self.cornersImage = list()
        self.reprojectionErrors = list()
        
    
    def generateMonoview(self, np_image, observation, obs_valid):

        np_image = cv2.cvtColor(np_image, cv2.COLOR_GRAY2BGR)

        if obs_valid:
            #calculate the reprojection error statistics
            cornersImage = observation.getCornersImageFrame();
            cornersReproj = observation.getCornerReprojection(self.camera.geometry);
            
            reprojectionErrors2 = cornersImage-cornersReproj
            reprojectionErrors = np.sum(np.abs(reprojectionErrors2)**2, axis=-1)**(1./2)
            
            #save the errors for reprojection error map plotting
            self.cornersImage.append(cornersImage)
            self.reprojectionErrors.append(reprojectionErrors)
            
            outputList = [ ( "mean_x:  ", np.mean(reprojectionErrors2[:,0]) ),
                           ( "std_x:   ", np.std(reprojectionErrors2[:,0]) ),
                           ( "max_y:   ", np.max(reprojectionErrors2[:,0]) ),
                           ( "min_x:   ", np.min(reprojectionErrors2[:,0]) ),
                           ( "", 0),
                           ( "mean_y:  ", np.mean(reprojectionErrors2[:,1]) ),
                           ( "std_y:   ", np.std(reprojectionErrors2[:,1]) ),
                           ( "max_y:   ", np.max(reprojectionErrors2[:,1]) ),
                           ( "min_y:   ", np.min(reprojectionErrors2[:,1]) ),
                           ( "", 0),
                           ( "mean_L2: ", np.mean(reprojectionErrors) ),
                           ( "std_L2:  ", np.std(reprojectionErrors) ),
                           ( "max_L2:  ", np.max(reprojectionErrors) ),
                           ( "min_L2:  ", np.min(reprojectionErrors) )      ]

            self.total_mean_reprojection_error_x += abs(np.mean(reprojectionErrors2[:,0]))
            self.total_std_reprojection_error_x += abs(np.std(reprojectionErrors2[:,0]))
            self.total_mean_reprojection_error_y += abs(np.mean(reprojectionErrors2[:,1]))
            self.total_std_reprojection_error_y += abs(np.std(reprojectionErrors2[:,1]))
            self.total_images += 1
    

            #print the text
            y = 20; x = 20
            for err_txt, err_val in outputList:
                fontScale = 0.75
                y += int(42*fontScale)
                
                if err_txt == "":
                    continue
                
                cv2.putText(np_image, err_txt, (x,y), cv2.FONT_HERSHEY_SIMPLEX, fontScale=fontScale, color=(0, 0, 255), thickness=2)
                cv2.putText(np_image, "{0: .4f}".format(err_val), (x+100,y), cv2.FONT_HERSHEY_SIMPLEX, fontScale=fontScale, color=(0, 0, 255), thickness=2)
            
            #draw reprojected corners
            for px, py in zip(cornersReproj[:,0],cornersReproj[:,1]):
                #convert pixel to fixed point (opencv subpixel rendering...)
                shift = 4; radius = 0.5; thickness = 1
                px_fix =  int(px * 2**shift)
                py_fix =  int(py * 2**shift)
                radius_fix = int(radius * 2**shift)
                cv2.circle(np_image, (px_fix, py_fix), radius=radius_fix, color=(255,255,0), thickness=thickness, shift=shift, lineType=cv2.LINE_AA)

        else:
            cv2.putText(np_image, "Detection failed...", (int(np_image.shape[0]/2.0),int(np_image.shape[1]/5.0)), cv2.FONT_HERSHEY_SIMPLEX, fontScale=1.5, color=(0, 0, 255), thickness=2)

        cv2.imshow(self.windowName, np_image)
        cv2.waitKey(1)


if __name__ == "__main__":        
    parser = argparse.ArgumentParser(description='Validate the intrinsics of a camera.')
    parser.add_argument('--target', dest='targetYaml', help='Calibration target configuration as yaml file', required=True)
    parser.add_argument('--cam', dest='chainYaml', help='Camera configuration as yaml file', required=True)
    parser.add_argument('--verbose', action='store_true', dest='verbose', help='Verbose output')
    parser.add_argument('--feature', dest='feature', help='Feature output')
    parsed = parser.parse_args()
        
    if parsed.verbose:
        sm.setLoggingLevel(sm.LoggingLevel.Debug)
    else:
        sm.setLoggingLevel(sm.LoggingLevel.Info)

    # init the node
    rospy.init_node('kalibr_feature_uw_validator', anonymous=True)

    #load the configuration yamls
    targetConfig = kc.ConfigReader.CalibrationTargetParameters(parsed.targetYaml)

    print("Initializing calibration target:".format(targetConfig.getTargetType()))
    targetConfig.printDetails()
    camchain = kc.ConfigReader.CameraChainParameters(parsed.chainYaml)
        
    #create the validator
    chain_validator = CameraChainValidator(camchain, targetConfig, parsed.feature)
    
    #ros message loops
    try:
        rospy.spin()
    except KeyboardInterrupt:
        print("Shutting down")
    
    cv2.destroyAllWindows()
